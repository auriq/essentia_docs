.. |<br>| raw:: html

   <br>

======
loginf
======

Log analyzer


Synopsis
========

::

  loginf [-h] Global_Opt Input_Spec Output_Spec

  Global_Opt:
      [-verb]
      [-cmax ColNumMax]

  Input_Spec:
      [-f[,AtrLst] File [File ...]] [-d ColId [ColId ...]]
      [-f_raw File [File ...]]

  Output_Spec:
      [-pp_eok Percent]
      [-o File [-b64]]
      [-o_raw File]
      [-o_pp_col File]


Description
===========

``loginf`` is used to collect data characteristics related information
from the input. It can process log files and/or previously generated
raw result files as input.
On output, it produces a data characteristics report in a JSON text format
and/or a raw result format.
Characteristics compiled include row counts, column counts,
column min/max length/value, column type guesses,
column uniquness estimates, and so on.


Options
=======

.. _`-verb`:

``-verb``
  Verbose - print program progress to stderr while processing.
  Usually, a marker is printed for each 10,000,000 records processed.


.. _`-cmax`:

``-cmax ColNumMax``
  The maximum number of columns to process. Default is 4096.
  Processing will stop if this limit is exceeded.


.. _`-f`:

``-f[,AtrLst] File [File ...] [-lim Num]``
  Set the input attributes and log files to analyze.
  If the data come from stdin, set ``File`` to '-' (a single dash).
  If no `-f`_ or `-f_raw`_ is given, log from stdin is assumed.
  Optional ``AtrLst`` is a list of comma separated attributes:

  * ``+Num[b|l]`` - Specifies the number of bytes (``b`` suffix)
    or lines (no suffix or ``l`` suffix) to skip before processing.
  * ``bz=BufSize`` - Set the per-record buffer size to ``BufSize`` bytes.
    It must be big enough to hold the data of all the columns in a record.
    Default size is 64KB.
  * ``notitle`` - The first record from the input is *not* a label line.
  * ``csv`` - Input is in CSV format. This is the default.
  * ``sep=c`` or ``sep=\xHH`` - Input is in 'c' (single byte) separated value
    format. '\xHH' is a way to specify 'c' via its HEX value ``HH``.
  * ``tab`` - Input is in HTML table format. Each row has the form
    "``...<td>Column1</td>...<td>Column2</td>...</tr>``".
    In other words, a row begins at the first "``<td ...>``" tag and
    ends at a "``</tr>``" tag.
  * ``rx=RecLimit`` - Set the maximum number of records to process.
  * ``auto`` - Determine input data format automatically.
    Supported formats are:

    * Delimiter-separated columns. May not work if the number of columns
      in not fixed.
    * Blank padded fixed-width columns. Individual columns
      can be left or right adjusted (but not both on the same column).
    * JSON, detection only, no further analysis.
    * XML, detection only, no further analysis.
    * Default to a line separated format with a single column.

  The ``-lim`` option sets the maximum number of records to load from *each*
  input file to ``Num``.

  Example:

   ::

    $ loginf ... -f file1 file2 ...

  * Load and analyze logs file1 and file2.


.. _`-d`:

``-d ColId [ColId ...]``
  Select the columns to analyze. Other columns will be ignored.
  ``ColId`` is one-based.


.. _`-f_raw`:

``-f_raw File [File ...]``
  Set the input raw result files to load.
  ``Files`` must be previously generated by this program via the
  `-o_raw`_ option.
  If the data come from stdin, set ``File`` to '-' (a single dash).

  Example:

   ::

    $ loginf ... -f_raw file1.raw file2.raw ...

  * Load and combine file1.raw and file2.raw.

   ::

    $ loginf ... -f file3 file4 -f_raw file1.raw file2.raw ...

  * Load and combine file1.raw and file2.raw, then further load and analyze logs
    file3 and file4 and combine all the results together.


.. _`-pp_eok`:

``-pp_eok Percent``
  Acceptable error percentage when determining column data type. Default is 0.
  Column data type is determined based on the column values. If more than one
  types are detected in a column, the type detected the most will be chosen
  if the percentage of all the other types combined is less than or equal to
  this threshold. Otherwise, a string type will be assigned when there is an
  inconsistency.


.. _`-o`:

``-o File [-b64]``
  Output a text report of the result.
  Report is written in JSON format.
  If ``File`` is a '-' (a single dash), data will be written to stdout.
  Note that the file will be overwritten if it contains any data.
  If no `-o`_, `-o_raw`_ or `-o_pp_col`_ is given, a report will be written
  to stdout.

  With the ``-b64`` option, the strings in the JSON report will be encoded
  in a base64 format.

  Example:

   ::

    $ loginf ... -f file1 ... -o file1.report

  * Save the JSON report to file1.report.


.. _`-o_raw`:

``-o_raw File``
  Output raw result.
  This raw result can be used in a later run using the `-f_raw`_ option.
  If ``File`` is a '-' (a single dash), data will be written to stdout.

  Example:

   ::

    $ loginf ... -f file1 ... -o_raw file1.raw -o file1.report

  * Save raw result to file1.raw and a report of the same result to
    file1.report.


.. _`-o_pp_col`:

``-o_pp_col File``
  Output aq_pp column spec based on the charasteristics of the processed data.
  The output is line oriented, with one column spec per line.
  If ``File`` is a '-' (a single dash), data will be written to stdout.

  Example:

   ::

    $ loginf ... -f file1 -lim 1000 ... -o_pp_col file1.col

  * Analyze the first 1000 records in file1 and output aq_pp column spec to
    file1.col.


Exit Status
===========

If successful, the program exits with status 0. Otherwise, the program exits
with a non-zero status code along error messages printed to stderr.
Applicable exit codes are:

* 0 - Successful.
* 1 - Memory allocation error.
* 2 - Command option spec error.
* 3 - Initialization error.
* 4 - System error.
* 5 - Missing or invalid license.
* 11 - Input open error.
* 12 - Input read error.
* 13 - Input processing error.
* 21 - Output open error.
* 22 - Output write error.


See Also
========

* `aq_pp <aq_pp.html>`_ - Record preprocessor
* `udbd <udbd.html>`_ - Udb server
* `aq_udb <aq_udb.html>`_ - Udb server interface

